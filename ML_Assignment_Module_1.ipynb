{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment Questions"
      ],
      "metadata": {
        "id": "Azm3ttmAkdhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1 - What is a parameter?\n",
        "\n",
        "A parameter is a value that helps define or control a system, function, or model. It serves as an input that influences how something operates."
      ],
      "metadata": {
        "id": "P6S2zP4dkhhU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2 - What is correlation?\n",
        "What does negative correlation mean?\n",
        "\n",
        "Correlation\n",
        "\n",
        "Correlation measures the relationship between two variables. It ranges from -1 to +1:\n",
        "\n",
        "+1 → Strong positive correlation (both increase or decrease together).\n",
        "0 → No correlation.\n",
        "-1 → Strong negative correlation (one increases, the other decreases).\n",
        "\n",
        "Negative Correlation\n",
        "\n",
        "A negative correlation means that as one variable increases, the other decreases (inverse relationship).\n",
        "\n",
        "Examples:\n",
        "\n",
        "More exercise → Lower body weight.\n",
        "Higher temperature → Lower hot coffee sales.\n",
        "Faster speed → Less travel time."
      ],
      "metadata": {
        "id": "8UgHk9UUkrzj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3 - Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "Machine Learning (ML) is a branch of artificial intelligence (AI) that enables computers to learn from data and make predictions or decisions without being explicitly programmed. It uses algorithms to identify patterns and improve performance over time.\n",
        "\n",
        "Main Components of Machine Learning\n",
        "\n",
        "1)Data – The foundation of ML; includes training and testing datasets.\n",
        "\n",
        "2)Features – Relevant attributes or characteristics extracted from data for learning.\n",
        "\n",
        "3)Model – The mathematical representation of the learned patterns.\n",
        "\n",
        "4)Algorithm – The method used to train the model (e.g., Decision Trees, Neural Networks).\n",
        "\n",
        "5)Training Process – The phase where the model learns from data.\n",
        "\n",
        "6)Evaluation – Assessing model performance using metrics like accuracy or loss.\n",
        "\n",
        "7)Prediction/Inference – Using the trained model to make decisions on new data.\n",
        "\n",
        "In short, ML involves feeding data into models that learn and improve to make accurate predictions."
      ],
      "metadata": {
        "id": "uHUghwJBk_yP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4 - How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "Loss Value in Model Evaluation\n",
        "\n",
        "The loss value measures how far a model's predictions are from actual values.\n",
        "\n",
        "How It Helps?\n",
        "\n",
        "Low Loss → Good model performance.\n",
        "High Loss → Poor predictions.\n",
        "Too Low Loss → Possible overfitting (memorizing data, not generalizing).\n",
        "\n",
        "Types of Loss\n",
        "\n",
        "Regression: MSE, MAE.\n",
        "Classification: Cross-Entropy Loss, Hinge Loss.\n"
      ],
      "metadata": {
        "id": "Md8NivXSlar5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5 - What are continuous and categorical variables?\n",
        "\n",
        "1. Continuous Variables\n",
        "Can take any numerical value within a range.\n",
        "Measured on a continuous scale (e.g., height, weight, temperature).\n",
        "Example: Height = 5.8 ft, Temperature = 36.5°C\n",
        "\n",
        "2. Categorical Variables\n",
        "Represent distinct categories or groups.\n",
        "Can be nominal (no order, e.g., colors) or ordinal (ordered, e.g., education level).\n",
        "Example: Gender (Male/Female), Education Level (High School, College, PhD)"
      ],
      "metadata": {
        "id": "CsuuduJqlxxm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6 - How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "Handling Categorical Variables in ML\n",
        "Since ML models need numerical data, categorical variables must be encoded.\n",
        "\n",
        "Common Techniques\n",
        "\n",
        "1)Label Encoding → Assigns a unique number (e.g., Red = 0, Blue = 1).\n",
        "\n",
        "2)One-Hot Encoding → Creates binary columns (e.g., Red → [1,0,0]).\n",
        "\n",
        "3)Ordinal Encoding → Assigns numbers while keeping order (Low = 0, Medium = 1).\n",
        "\n",
        "4)Frequency Encoding → Replaces categories with their count.\n",
        "\n",
        "5)Target Encoding → Replaces categories with the mean of the target variable."
      ],
      "metadata": {
        "id": "hRnzHf3BmEUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7 - What do you mean by training and testing a dataset?\n",
        "\n",
        "In Machine Learning, data is split into two main parts:\n",
        "\n",
        "1)Training Dataset\n",
        "\n",
        "Used to train the model by learning patterns from the data.\n",
        "The model adjusts parameters to minimize error.\n",
        "Typically 70-80% of the total dataset.\n",
        "\n",
        "2)Testing Dataset\n",
        "\n",
        "Used to evaluate the model’s performance on unseen data.\n",
        "Helps check if the model generalizes well.\n",
        "Typically 20-30% of the total dataset."
      ],
      "metadata": {
        "id": "r7U6IJoXmofC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8 - What is sklearn.preprocessing?\n",
        "\n",
        "sklearn.preprocessing is a module in Scikit-Learn that provides tools for scaling, encoding, and transforming data to improve model performance.\n",
        "\n",
        "Common Functions:\n",
        "StandardScaler → Standardizes data (mean = 0, variance = 1).\n",
        "MinMaxScaler → Scales data to a specific range (e.g., 0 to 1).\n",
        "LabelEncoder → Converts categorical labels into numeric values.\n",
        "OneHotEncoder → Converts categorical features into binary columns.\n",
        "Binarizer → Converts values into 0s and 1s based on a threshold."
      ],
      "metadata": {
        "id": "EUTFYi-ynEQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9 - What is a Test set?\n",
        "\n",
        "A test set is a portion of the dataset used to evaluate the performance of a trained machine learning model.\n",
        "\n",
        "Key Points:\n",
        "It is not used for training the model.\n",
        "Helps measure how well the model generalizes to new, unseen data.\n",
        "Typically 20-30% of the total dataset.\n",
        "Used to compute metrics like accuracy, precision, recall, or RMSE."
      ],
      "metadata": {
        "id": "vmpHoOYlnd3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10 - How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?\n",
        "\n",
        "1. Splitting Data for Training & Testing in Python\n",
        "Use train_test_split from sklearn.model_selection:\n",
        "\n",
        "from sklearn.model_selection import train_test_split  \n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  \n",
        "\n",
        "test_size=0.2 → 20% for testing, random_state=42 → Ensures reproducibility.\n",
        "\n",
        "2. Approach to Solving an ML Problem\n",
        "✅ Step 1: Understand the Problem → Define the goal & key variables.\n",
        "✅ Step 2: Data Collection & Preprocessing → Handle missing values, encode categorical data, normalize if needed.\n",
        "✅ Step 3: Split Data → Use train_test_split.\n",
        "✅ Step 4: Model Selection & Training → Choose an ML algorithm & train the model.\n",
        "✅ Step 5: Model Evaluation → Test using accuracy, MSE, precision, recall, etc.\n",
        "✅ Step 6: Hyperparameter Tuning → Optimize model parameters.\n",
        "✅ Step 7: Deployment & Monitoring → Deploy and monitor for improvements."
      ],
      "metadata": {
        "id": "vnXHSErgnxyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11 - Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "Why Perform EDA Before Model Fitting?\n",
        "Exploratory Data Analysis (EDA) helps understand the dataset before training a machine learning model. It ensures better model performance and prevents errors.\n",
        "\n",
        "Key Reasons for EDA:\n",
        "1)Understand Data Distribution → Helps check patterns, trends, and relationships.\n",
        "2)Detect Missing Values → Identify and handle missing data (e.g., imputation or removal).\n",
        "3)Identify Outliers → Remove or treat extreme values that may distort model training.\n",
        "4)Feature Selection → Identify important variables and drop irrelevant ones.\n",
        "5)Check Correlations → Detect multicollinearity between features.\n",
        "6)Data Transformation → Normalize, scale, or encode categorical variables for better model compatibility."
      ],
      "metadata": {
        "id": "vrF--R-XoUTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12 - What is correlation?\n",
        "\n",
        "Correlation measures the relationship between two variables, indicating how one variable changes concerning another.\n",
        "\n",
        "Correlation Coefficient (r) Range:\n",
        "\n",
        "+1 → Perfect positive correlation (both increase together).\n",
        "\n",
        "0 → No correlation (no relationship between variables).\n",
        "\n",
        "-1 → Perfect negative correlation (one increases, the other decreases)."
      ],
      "metadata": {
        "id": "fCaGXpUkosTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13 - What does negative correlation mean?\n",
        "\n",
        "A negative correlation means that as one variable increases, the other decreases (inverse relationship).\n",
        "\n",
        "Correlation Coefficient (r) for Negative Correlation:\n",
        "\n",
        "r = -1 → Perfect negative correlation (strong inverse relationship).\n",
        "\n",
        "r = 0 → No correlation.\n",
        "\n",
        "r between -1 and 0 → Weak to moderate negative correlation."
      ],
      "metadata": {
        "id": "UCOwz9Uto31d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14 - How can you find correlation between variables in Python?\n",
        "\n",
        "✅ Using Pandas (corr()) → For correlation between multiple variables\n",
        "import pandas as pd  \n",
        "df = pd.DataFrame({'A': [1, 2, 3], 'B': [2, 4, 6], 'C': [10, 8, 6]})\n",
        "\n",
        "print(df.corr())  # Default is Pearson correlation\n",
        "\n",
        "✅ Using NumPy (corrcoef()) → For correlation between two variables\n",
        "\n",
        "import numpy as np  \n",
        "x = [1, 2, 3, 4, 5]\n",
        "\n",
        "y = [10, 8, 6, 4, 2]\n",
        "\n",
        "print(np.corrcoef(x, y)[0, 1])  \n",
        "\n",
        "Key Takeaway: df.corr() for multiple variables, np.corrcoef() for two variables."
      ],
      "metadata": {
        "id": "VXXv7vVNpP6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15 - What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "Causation refers to a direct cause-and-effect relationship between two variables, meaning that a change in one variable directly leads to a change in another. On the other hand, correlation simply means that two variables are related and tend to move together, but it does not mean that one causes the other. For example, ice cream sales and drowning rates are positively correlated because both increase during summer. However, ice cream sales do not cause drowning; the real cause is hot weather, a third factor influencing both variables. In contrast, smoking and lung cancer have a causal relationship—scientific studies have shown that smoking directly increases the risk of lung cancer. This distinction is crucial in data analysis and machine learning, as correlation alone cannot be used to infer causation without proper experimental evidence."
      ],
      "metadata": {
        "id": "Qd0DbBjCp-cD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16 - What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "An optimizer is an algorithm that updates a model’s parameters (weights and biases) to minimize the loss function and improve performance during training.\n",
        "\n",
        "Types of Optimizers (with Examples)\n",
        "1. Gradient Descent (GD)\n",
        "Updates weights based on the overall dataset.\n",
        "Example: Used in linear regression.\n",
        "Limitation: Slow for large datasets.\n",
        "2. Stochastic Gradient Descent (SGD)\n",
        "Updates weights after each sample (faster but noisier).\n",
        "Example: Used in deep learning with large datasets.\n",
        "3. Mini-Batch Gradient Descent\n",
        "Updates weights using small batches of data (balance of GD & SGD).\n",
        "Example: CNN training.\n",
        "4. Adam (Adaptive Moment Estimation)\n",
        "Combines momentum & RMSprop for adaptive learning.\n",
        "Example: Used in deep learning models like Transformers.\n",
        "5. RMSprop (Root Mean Square Propagation)\n",
        "Adjusts learning rates dynamically to stabilize training.\n",
        "Example: Used in RNNs.\n",
        "6. Adagrad (Adaptive Gradient Algorithm)\n",
        "Adapts learning rates for each parameter, useful for sparse data.\n",
        "Example: NLP tasks like word embeddings.\n",
        "7. Adadelta\n",
        "Improves Adagrad by limiting learning rate decay.\n",
        "Example: Used in image classification tasks."
      ],
      "metadata": {
        "id": "n8b0MSikqYBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17 - What is sklearn.linear_model ?\n",
        "\n",
        "sklearn.linear_model is a Scikit-Learn module that provides various linear models for regression and classification.\n",
        "\n",
        "Common Models:\n",
        "✅ Linear Regression (LinearRegression) → Predicts continuous values.\n",
        "✅ Logistic Regression (LogisticRegression) → Used for classification.\n",
        "✅ Ridge Regression (Ridge) → Linear regression with L2 regularization (reduces overfitting).\n",
        "✅ Lasso Regression (Lasso) → Linear regression with L1 regularization (feature selection).\n",
        "\n",
        "💡 Key Takeaway: sklearn.linear_model is useful for ML tasks involving linear relationships!"
      ],
      "metadata": {
        "id": "JkAEKfztq8fz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18 - What does model.fit() do? What arguments must be given?\n",
        "\n",
        "What Does model.fit() Do?\n",
        "model.fit() trains the model by learning patterns from the given data and adjusting parameters to minimize errors.\n",
        "\n",
        "Required Arguments:\n",
        "✅ X (Input Features) → Independent variables (e.g., NumPy array, DataFrame).\n",
        "✅ y (Target/Labels) → Dependent variable (output values).\n",
        "\n",
        "Example:\n",
        "\n",
        "model.fit(X, y)  # X = features, y = target"
      ],
      "metadata": {
        "id": "Qge1zEbSrMm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19 - What does model.predict() do? What arguments must be given?\n",
        "\n",
        "model.predict() makes predictions using the trained model based on new input data.\n",
        "\n",
        "Required Argument:\n",
        "✅ X_new (New Input Data) → The features for which predictions are needed.\n",
        "\n",
        "Example:\n",
        "y_pred = model.predict(X_new)  # Predicts output for new X values"
      ],
      "metadata": {
        "id": "vNBNABhZrjAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20 - What are continuous and categorical variables?\n",
        "\n",
        "Continuous variables are numerical values that can take any value within a range, such as height, weight, or temperature. They are measured on a continuous scale and can have decimal values. Categorical variables, on the other hand, represent distinct groups or categories. They can be nominal (no order, e.g., colors: Red, Blue) or ordinal (ordered, e.g., education levels: High School, College). In machine learning, categorical variables often need encoding (e.g., One-Hot Encoding) before use in models, while continuous variables may require scaling for better performance."
      ],
      "metadata": {
        "id": "JMjolpqprxPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q21 - What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "What is Feature Scaling?\n",
        "Feature scaling is the process of normalizing or standardizing numerical data to ensure all features have a similar scale.\n",
        "\n",
        "How Does It Help in ML?\n",
        "✅ Improves Model Performance → Prevents certain features from dominating others.\n",
        "✅ Speeds Up Convergence → Helps gradient-based models (e.g., Logistic Regression, Neural Networks) train faster.\n",
        "✅ Required for Distance-Based Models → Algorithms like KNN, SVM, and K-Means perform better with scaled data.\n",
        "\n",
        "Common Scaling Methods:\n",
        "Standardization (StandardScaler) → Scales data to mean = 0 and variance = 1.\n",
        "Normalization (MinMaxScaler) → Scales values between 0 and 1."
      ],
      "metadata": {
        "id": "QVrd5eMZsAeN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q22 - How do we perform scaling in Python?\n",
        "\n",
        "Use Scikit-Learn's StandardScaler or MinMaxScaler for scaling.\n",
        "\n",
        "1. Standardization (Mean = 0, Variance = 1)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler  \n",
        "\n",
        "scaler = StandardScaler()  \n",
        "X_scaled = scaler.fit_transform(X)  \n",
        "\n",
        "2. Normalization (Scales Between 0 and 1)\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler  \n",
        "\n",
        "scaler = MinMaxScaler()  \n",
        "X_scaled = scaler.fit_transform(X)  \n"
      ],
      "metadata": {
        "id": "bxoDwcXgsYlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q23 - What is sklearn.preprocessing?\n",
        "\n",
        "sklearn.preprocessing is a Scikit-Learn module that provides tools for scaling, encoding, and transforming data to improve machine learning model performance.\n",
        "\n",
        "Common Functions:\n",
        "\n",
        "✅ Feature Scaling:\n",
        "\n",
        "StandardScaler() → Standardizes data (mean = 0, variance = 1).\n",
        "\n",
        "MinMaxScaler() → Normalizes data between 0 and 1.\n",
        "\n",
        "✅ Encoding Categorical Data:\n",
        "\n",
        "LabelEncoder() → Converts categories into numerical values.\n",
        "\n",
        "OneHotEncoder() → Creates binary columns for each category.\n",
        "\n",
        "✅ Other Transformations:\n",
        "\n",
        "Binarizer() → Converts values into 0s and 1s.\n",
        "\n",
        "PolynomialFeatures() → Generates polynomial features for models."
      ],
      "metadata": {
        "id": "7WhFxMoKswRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q24 - How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "How to Split Data for Model Fitting in Python?\n",
        "Use train_test_split from Scikit-Learn to split data into training and testing sets.\n",
        "\n",
        "from sklearn.model_selection import train_test_split  \n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  \n",
        "\n",
        "Key Parameters:\n",
        "✅ test_size=0.2 → 20% data for testing, 80% for training.\n",
        "✅ random_state=42 → Ensures reproducibility."
      ],
      "metadata": {
        "id": "jJHZsX0wtDgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q25 - Explain data encoding?\n",
        "\n",
        "Data encoding converts categorical data into numerical format so ML models can process it.\n",
        "\n",
        "Types of Encoding:\n",
        "\n",
        "✅ Label Encoding → Assigns unique numbers to categories.\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder  \n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "y_encoded = le.fit_transform(['Red', 'Blue', 'Green'])  # [2, 0, 1]\n",
        "\n",
        "✅ One-Hot Encoding → Creates binary columns for each category.\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "import pandas as pd  \n",
        "\n",
        "ohe = OneHotEncoder(sparse=False)  \n",
        "\n",
        "encoded = ohe.fit_transform(pd.DataFrame(['Red', 'Blue', 'Green']))  \n",
        "\n"
      ],
      "metadata": {
        "id": "b2yxuA6PtUDR"
      }
    }
  ]
}